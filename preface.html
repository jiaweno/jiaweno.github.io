
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://jiaweno.github.io/preface.html">
      
      
        <link rel="prev" href="index.html">
      
      
        <link rel="next" href="%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E8%A7%A3%E9%87%8A.html">
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>✅前言 - My Docs</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="index.html" title="My Docs" class="md-header__button md-logo" aria-label="My Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ✅前言
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="My Docs" class="md-nav__button md-logo" aria-label="My Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    首页
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    1.大模型基础
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            1.大模型基础
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    ✅前言
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="preface.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    ✅前言
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      📖 目录
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 引言
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      2. Transformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-llms" class="md-nav__link">
    <span class="md-ellipsis">
      3. LLMs训练流程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. LLMs训练流程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-pre-training" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 预训练（Pre-training）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-sft" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 监督微调（SFT）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 强化学习（RLHF）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-llms" class="md-nav__link">
    <span class="md-ellipsis">
      4. LLMs核心技术
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. LLMs核心技术">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 词元（Tokens）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 嵌入表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 上下文窗口
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 推理采样策略
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45-prompts" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 提示工程（Prompts）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 多模态
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-ai-agents" class="md-nav__link">
    <span class="md-ellipsis">
      6. AI Agents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 大模型当前的局限性
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 大模型未来的展望
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9" class="md-nav__link">
    <span class="md-ellipsis">
      9. 结论
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 参考文献与进阶阅读
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E8%A7%A3%E9%87%8A.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ✅大模型专业术语解释
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      📖 目录
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 引言
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      2. Transformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-llms" class="md-nav__link">
    <span class="md-ellipsis">
      3. LLMs训练流程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. LLMs训练流程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-pre-training" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 预训练（Pre-training）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-sft" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 监督微调（SFT）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 强化学习（RLHF）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-llms" class="md-nav__link">
    <span class="md-ellipsis">
      4. LLMs核心技术
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. LLMs核心技术">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 词元（Tokens）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 嵌入表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 上下文窗口
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 推理采样策略
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45-prompts" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 提示工程（Prompts）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 多模态
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-ai-agents" class="md-nav__link">
    <span class="md-ellipsis">
      6. AI Agents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 大模型当前的局限性
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 大模型未来的展望
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9" class="md-nav__link">
    <span class="md-ellipsis">
      9. 结论
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 参考文献与进阶阅读
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="_1">大语言模型深度解析</h1>
<blockquote>
<p>本文章为Gemini 2.5 Pro生成，文章内容基本来自论文，比较权威。</p>
<p>其中，2，3，4点内容是比较基础的技术概括，5，6，7，8点内容是作者根据当前AI热门发展内容的总结，感兴趣的读者可以跳转至相应的内容。</p>
<p>文章更新于：2025.5.22</p>
</blockquote>
<h2 id="_2">📖 目录</h2>
<ul>
<li><a href="#1-引言">1. 引言</a></li>
<li><a href="#2-transformer">2. Transformer</a></li>
<li><a href="#3-llms训练流程">3. LLMs训练流程</a></li>
<li><a href="#4-llms核心技术">4. LLMs核心技术</a></li>
<li><a href="#5-多模态">5. 多模态</a></li>
<li><a href="#6-ai-agents">6. AI Agents</a></li>
<li><a href="#7-大模型当前的局限性">7. 大模型当前的局限性</a></li>
<li><a href="#8-大模型未来的展望">8. 大模型未来的展望</a></li>
<li><a href="#9-结论">9. 结论</a></li>
<li><a href="#10-参考文献与进阶阅读">10. 参考文献与进阶阅读</a></li>
</ul>
<h2 id="1">1. 引言</h2>
<p>自2022年末以来，以ChatGPT为代表的大语言模型（Large Language Models, LLMs）引发了全球范围内的科技浪潮，标志着人工智能（Artificial Intelligence, AI）进入了一个新的发展阶段。相较于此前专注于特定任务（如图像识别、棋类博弈）的AI系统，大语言模型展现出在自然语言理解、生成、以及多种复杂认知任务上的通用能力，迅速成为AI研究与应用的核心。本文旨在为对大模型已有初步了解但希望构建更清晰、系统性认知的读者，提供一份深度解析，涵盖其核心原理、关键技术、应用现状及未来趋势。</p>
<h2 id="2-transformer">2. Transformer</h2>
<p>Transformer 是一种深度学习模型架构，最初由Google的研究团队在2017年的论文《<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>》中提出。它彻底改变了自然语言处理（NLP）领域，展现出相较于传统循环神经网络（RNN）和长短期记忆网络（LSTM）的显著优势，并成为许多现代AI模型（如GPT、BERT等）的核心基础。</p>
<p>Transformer的核心创新在于其<strong>自注意力机制（Self-Attention Mechanism）</strong>。该机制允许模型在处理序列中的任一元素（词元）时，能够并行地计算该元素与序列中所有其他元素之间的关联程度（即“注意力权重”），并据此生成该元素的上下文感知表征。这种并行处理特性极大地提高了训练效率，并能有效捕捉文本中的长距离依赖关系，克服了RNN难以并行化和长程依赖捕捉不足的瓶颈。</p>
<p>其数学表达的核心思想可以简化为：对于一个查询Q（Query）、一组键K（Key）和一组值V（Value），注意力输出计算如下： 
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<p>其中，dk是键向量的维度，用于缩放点积结果，防止梯度过小。Transformer进一步引入了<strong>多头注意力（Multi-Head Attention）</strong>，允许模型在不同的表示子空间中并行地学习不同方面的注意力信息，然后将这些信息拼接并线性变换得到最终输出，从而增强了模型的表达能力。</p>
<p><img alt="transformer-en" src="images/transformer-en.png" /></p>
<p>由于自注意力机制本身不处理序列顺序，Transformer引入了<strong>位置编码（Positional Encoding）</strong>，将词元在序列中的位置信息以向量形式注入到输入嵌入中，使模型能够感知词序。</p>
<p>Transformer架构凭借其卓越的性能和可扩展性，迅速成为自然语言处理领域的主流模型架构，为后续BERT、GPT系列等众多影响力深远的大语言模型奠定了基础。</p>
<h2 id="3-llms">3. LLMs训练流程</h2>
<p>大语言模型的强大能力源自于其精心设计的训练流程，该流程通常包含预训练、监督微调和基于人类反馈的强化学习三个主要阶段。</p>
<h3 id="31-pre-training">3.1 预训练（Pre-training）</h3>
<p>预训练是大模型构建中最为关键且资源消耗最大的环节。在这一阶段，模型在海量的、多样化的未标注文本及代码数据上进行学习。预训练的目标是让模型掌握语言的统计规律、语法结构、语义知识以及广泛的世界常识。</p>
<p>主要的预训练任务包括：</p>
<ul>
<li><strong>掩码语言建模（Masked Language Modeling, MLM）</strong>：如BERT采用的策略，随机遮蔽输入文本中的部分词元，训练模型根据上下文预测被遮蔽的词元。</li>
<li><strong>因果语言建模（Causal Language Modeling, CLM）</strong> 或 <strong>下一词元预测（Next Token Prediction）</strong>：如GPT系列采用的策略，模型根据给定的上文预测序列中的下一个词元。这种方式使模型天然具备文本生成能力。</li>
</ul>
<p>通过这些自监督学习任务，模型从大规模数据中自主学习语言的深层表示，形成一个通用的<strong>基础模型（Foundation Model）</strong>。</p>
<h3 id="32-sft">3.2 监督微调（SFT）</h3>
<p>预训练后的基础模型虽然具备广泛的语言知识，但其行为模式可能与人类的期望或特定任务的需求不完全一致。监督微调旨在通过有监督学习的方式，使模型适应特定的指令遵循模式或下游任务。</p>
<p>SFT阶段使用一个规模相对较小但质量较高的标注数据集。该数据集由成对的“指令-期望输出”（Prompt-Completion）样本构成，这些样本通常由人工编写或经过严格筛选。模型在这些高质量样本上进行微调，学习如何根据指令生成有用、准确且无害的回答。</p>
<h3 id="33-rlhf">3.3 强化学习（RLHF）</h3>
<p>为进一步提升模型的输出质量，使其更好地对齐人类的复杂偏好和价值观，并减少生成有害或失实内容的倾向，RLHF被广泛应用。该过程主要包括：</p>
<ol>
<li><strong>奖励模型训练（Reward Model Training）</strong>：</li>
<li>收集数据：针对同一指令，让SFT模型生成多个候选回答。</li>
<li>人工排序/评分：由人类评估者对这些候选回答进行排序或给出偏好评分。</li>
<li>模型训练：利用这些人工反馈数据，训练一个奖励模型（RM）。该模型能够接收一个“指令-回答”对，并输出一个标量分数，量化该回答符合人类偏好的程度。</li>
<li><strong>强化学习优化（Reinforcement Learning Optimization）</strong>：</li>
<li>策略定义：将SFT模型或经过初步RLHF优化的模型作为强化学习中的策略网络。</li>
<li>交互与反馈：策略网络根据输入指令生成回答，奖励模型对该回答进行评分，此评分作为强化学习的奖励信号。</li>
<li>策略更新：使用强化学习算法（如Proximal Policy Optimization, PPO）更新策略网络（即语言模型）的参数，目标是最大化累积期望奖励，从而引导模型生成更受人类偏好的内容。</li>
</ol>
<p><img alt="model-tuning" src="images/model-tuning.png" /></p>
<p>RLHF的引入，显著提升了如ChatGPT等模型在对话连贯性、指令遵循能力和安全性方面的表现。</p>
<h2 id="4-llms">4. LLMs核心技术</h2>
<p>理解大模型的工作机制，需要掌握一些核心的技术概念：</p>
<h3 id="41-tokens">4.1 词元（Tokens）</h3>
<p>文本在输入模型前，需通过词元化器（Tokenizer）将其分割为模型可处理的基本单元——词元（Token）。词元可以是单词、子词（如BPE、WordPiece或SentencePiece算法生成的单元）或字符。子词词元化策略能够在控制词表规模的同时，有效处理未登录词（OOV）并保留一定的形态学信息。</p>
<h3 id="42">4.2 嵌入表示</h3>
<p>词元随后被映射为高维稠密的实数向量，即词元嵌入（Token Embeddings）。这些嵌入向量旨在捕捉词元的语义信息，使得语义相近的词元在向量空间中也更为接近。Transformer模型中，除了词元嵌入，还包括位置嵌入（Positional Embeddings）以表征词元在序列中的位置，以及可能的段落嵌入（Segment Embeddings）等。</p>
<h3 id="43">4.3 上下文窗口</h3>
<p>上下文窗口定义了模型在进行预测或生成时能够参考的先前词元的最大数量。例如，4K（4096个词元）的上下文窗口意味着模型在生成当前词元时，最多能利用其前4095个词元的信息。更长的上下文窗口通常能带来更好的长文本理解和生成能力，但也对计算资源和模型设计（如注意力机制的效率）提出更高要求。FlashAttention等技术致力于在扩展上下文长度的同时优化计算效率。</p>
<h3 id="44">4.4 推理采样策略</h3>
<p>在文本生成阶段，为了控制输出的多样性和确定性，常采用以下参数和策略：</p>
<ul>
<li><strong>温度（Temperature）</strong>：调节输出随机性的参数。较低温度使模型倾向于选择概率最高的词元，生成更确定、保守的内容；较高温度则增加选择低概率词元的可能性，生成更多样、创新的内容，但可能牺牲一定的连贯性。</li>
<li><strong>Top-k采样</strong>：在每一步生成时，仅从概率最高的k个候选词元中进行采样。</li>
<li><strong>Top-p（Nucleus）采样</strong>：选择一个累积概率阈值p，仅从概率之和刚好超过p的最小词元集合中进行采样。这种方法比Top-k更具适应性。</li>
</ul>
<p>合理的采样策略组合对于生成高质量且符合需求的文本至关重要。</p>
<h3 id="45-prompts">4.5 提示工程（Prompts）</h3>
<p>提示（Prompt）是用户与大模型交互的接口，其设计质量直接影响模型的输出效果。提示工程是指设计和优化提示以引导模型高效、准确地完成任务的艺术和科学。有效的提示应具备清晰性、明确性，并可包含任务描述、角色设定、上下文信息、少量示例（Few-shot Prompting）乃至思维链引导（Chain-of-Thought Prompting），后者通过引导模型输出中间推理步骤来提升复杂问题的解决能力。</p>
<h2 id="5">5. 多模态</h2>
<p>在最开始，ChatGPT 3.5就是一个聊天页面，用户输入文字，大模型返回文字；如果强制在提示词中要求大模型生成文件或媒体等内容，大模型会表示做不到。早期大模型主要聚焦于文本处理，但是到了现在，<strong>多模态大模型（Multimodal Large Models）</strong>已成为当前重要的发展方向，例如ChatGPT 4o，这类模型能够处理和生成如文本、图像、音频和视频这种类型的数据。</p>
<p>关键技术包括：</p>
<ul>
<li><strong>跨模态嵌入空间学习</strong>：将不同模态的信息映射到统一的语义表示空间，如CLIP模型通过对比学习对齐图像和文本的嵌入。</li>
<li><strong>多模态融合机制</strong>：设计有效的模块来融合来自不同模态的信息，如交叉注意力机制。</li>
</ul>
<p><img alt="Multimodal" src="images/Multimodal.png" /></p>
<p>除了大模型具有多模态的能力之外，许多厂商将文生图、文生视频的能力单独拎出来，形成独立的应用，本质还是用户输入Prompt，大模型再对生对应的内容，代表性应用有：</p>
<ul>
<li><strong>文本到图像生成</strong>：如DALL-E、Stable Diffusion、Imagen</li>
<li><strong>文本到视频生成</strong>：如fish audio、Azure TTS</li>
<li><strong>文本到视频生成</strong>：如Sora、Veo</li>
</ul>
<p>多模态能力的整合使得大模型能更全面地感知和交互，多模态能力是通往AGI（通用型人工智能）的必进之路，在《钢铁侠》中的“贾维斯”就是类似AGI的形态，如果他只能看懂文字和处理文字，那不完犊子了嘛。</p>
<h2 id="6-ai-agents">6. AI Agents</h2>
<p>大语言模型凭借其强大的通用性和适应性，已在众多领域展现出巨大的应用潜力：</p>
<ul>
<li><strong>信息获取与整合</strong>：智能搜索、文档摘要、知识问答。</li>
<li><strong>内容创作</strong>：辅助写作（邮件、报告、代码、营销文案、创意文本）。</li>
<li><strong>人机交互</strong>：智能客服、虚拟助手、情感陪伴。</li>
<li><strong>教育科研</strong>：个性化辅导、研究助理、数据分析。</li>
<li><strong>软件开发</strong>：代码生成、解释、调试和自动化测试（如GitHub Copilot）。</li>
<li><strong>专业服务</strong>：医疗领域的辅助诊断、病历分析；金融领域的市场分析、风险评估；法律领域的文书起草、案例检索。</li>
<li><strong>AI智能体（AI Agents）</strong>：赋予模型自主规划、决策和执行复杂任务的能力，通过调用工具（APIs、代码解释器等）与外部环境交互，实现如自动化数据分析、任务管理等。</li>
</ul>
<p>这些应用正在重塑各行各业的工作模式，并催生新的商业机会。</p>
<h2 id="7">7. 大模型当前的局限性</h2>
<p>尽管大模型取得了显著进展，但其发展与应用仍面临诸多挑战：</p>
<ul>
<li><strong>“幻觉”与事实一致性</strong>：模型可能生成看似合理但与事实不符或无意义的内容。确保输出的真实性和可靠性是核心挑战。</li>
<li><strong>数据偏见与公平性</strong>：训练数据中存在的偏见可能被模型学习并放大，导致不公平或歧视性的输出。</li>
<li><strong>可解释性与透明度</strong>：大模型决策过程的“黑箱”特性使得理解其行为和诊断错误变得困难。</li>
<li><strong>计算资源消耗与环境影响</strong>：训练和部署大规模模型需要巨大的计算能力和能源消耗。</li>
<li><strong>数据安全与隐私保护</strong>：处理敏感数据时，存在泄露和滥用风险。</li>
<li><strong>知识更新与时效性</strong>：预训练模型知识截止于训练数据，对新近信息的处理能力有限，需要通过检索增强生成（RAG）等技术弥补。</li>
<li><strong>滥用风险</strong>：如生成虚假信息、恶意代码、进行网络攻击等。</li>
</ul>
<p>应对这些挑战需要技术创新、行业自律以及健全的伦理规范和法律监管框架。</p>
<h2 id="8">8. 大模型未来的展望</h2>
<p>大语言模型领域的研究日新月异，未来发展趋势可能包括：</p>
<ul>
<li><strong>模型架构创新</strong>：探索超越Transformer的新型高效架构，如状态空间模型（SSMs）的变体（Mamba）。</li>
<li><strong>上下文长度的持续扩展</strong>：支持更长文档的理解和更连贯的多轮交互。</li>
<li><strong>推理能力的增强</strong>：提升模型在逻辑、数学、规划等复杂认知任务上的表现，可能结合符号推理方法。</li>
<li><strong>模型效率提升</strong>：通过模型压缩、量化、知识蒸馏、稀疏化（如混合专家模型MoE）等技术，降低部署和运行成本。</li>
<li><strong>世界模型与具身智能</strong>：构建能理解和预测物理世界动态的模型，并与机器人等实体结合，实现与物理世界的交互。</li>
<li><strong>自主学习与持续适应</strong>：发展能够从持续交互和新数据中高效学习并适应环境变化的能力。</li>
<li><strong>AI对齐（Alignment）研究的深化</strong>：确保模型的目标和行为与人类的价值观和意图高度一致，是实现安全可控AI的关键。</li>
<li><strong>向通用人工智能（AGI）的探索</strong>：大模型被认为是通往AGI的重要路径之一，未来将持续探索其认知能力的边界。</li>
<li><strong>[图片描述]</strong>：此处可插入一张示意图，描绘大模型未来发展的几个关键方向，如更强的推理能力、多模态深度融合、高效模型、AI安全与对齐等。</li>
</ul>
<h2 id="9">9. 结论</h2>
<p>大语言模型作为人工智能领域的一项变革性技术，正在深刻影响科学研究、产业发展和社会生活的方方面面。它们在自然语言处理及相关认知任务上展现的卓越能力，为解决复杂问题和提升生产力开辟了新的途径。然而，伴随其强大能力而来的挑战亦不容忽视。未来，通过持续的技术攻坚、跨学科协作以及负责任的伦理引导，大语言模型有望在推动社会进步和增进人类福祉方面发挥更加积极和深远的作用。对于希望深入理解并应用这一技术的个体而言，持续学习和批判性思考将是驾驭这场技术变革的关键。</p>
<h2 id="10">10. 参考文献与进阶阅读</h2>
<ol>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <strong>Attention is all you need</strong>. <em>Advances in neural information processing systems, 30</em>.</li>
<li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). <strong>Bert: Pre-training of deep bidirectional transformers for language understanding</strong>. <em>arXiv preprint arXiv:1810.04805</em>.</li>
<li>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <strong>Language models are unsupervised multitask learners</strong>. <em>OpenAI blog, 1(8), 9</em>. (GPT-2)</li>
<li>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020). <strong>Language models are few-shot learners</strong>. <em>Advances in neural information processing systems, 33, 1877-1901</em>. (GPT-3)</li>
<li>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... &amp; Lowe, R. (2022). <strong>Training language models to follow instructions with human feedback</strong>. <em>Advances in Neural Information Processing Systems, 35, 27730-27744</em>. (InstructGPT/RLHF)</li>
<li>OpenAI. (2023). <strong>GPT-4 Technical Report</strong>. <em>arXiv preprint arXiv:2303.08774</em>.</li>
<li>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... &amp; Lample, G. (2023). <strong>Llama 2: Open foundation and fine-tuned chat models</strong>. <em>arXiv preprint arXiv:2307.09288</em>.</li>
<li>Wei, J., Wang, X., Schuurmans, D., Bosma,M., Xia, F., Chi, E., ... &amp; Zhou, D. (2022). <strong>Chain-of-thought prompting elicits reasoning in large language models</strong>. <em>Advances in Neural Information Processing Systems, 35, 24824-24837</em>.</li>
<li>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... &amp; Sutskever, I. (2021). <strong>Learning transferable visual models from natural language supervision</strong>. <em>International conference on machine learning (pp. 8748-8763). PMLR</em>. (CLIP)</li>
<li>Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... &amp; Liang, P. (2021). <strong>On the opportunities and risks of foundation models</strong>. <em>arXiv preprint arXiv:2108.07258</em>.</li>
<li>Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., ... &amp; Wen, J. R. (2023). <strong>A survey of large language models</strong>. <em>arXiv preprint arXiv:2303.18223</em>. (一篇全面的综述性论文)</li>
</ol>
<p><strong>推荐学习资源：</strong></p>
<ul>
<li><strong>Stanford CS224N: NLP with Deep Learning</strong>: https://web.stanford.edu/class/cs224n/</li>
<li><strong>Hugging Face Courses</strong>: https://huggingface.co/learn</li>
<li><strong>AI Papers with Code</strong>: https://paperswithcode.com/area/natural-language-processing</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": [], "search": "assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>