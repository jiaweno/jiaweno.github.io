
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://jiaweno.github.io/other/QWQ32B.html">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>QWQ32B - My Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../index.html" title="My Docs" class="md-header__button md-logo" aria-label="My Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              QWQ32B
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="My Docs" class="md-nav__button md-logo" aria-label="My Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    首页
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    1.大模型基础
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            1.大模型基础
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preface.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ✅前言
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E8%A7%A3%E9%87%8A.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ✅大模型专业术语解释
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      一、模型加载相关
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      二、性能优化
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      三、采样控制
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#api" class="md-nav__link">
    <span class="md-ellipsis">
      四、API 服务配置
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      五、高级调试
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      六、多 GPU 优化示例
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      七、监控建议
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>QWQ32B</h1>

<p>【1】</p>
<pre><code class="language-Bash">conda create --name QWQ-llamacpp python=3.11
conda init
source ~/.bashrc
conda activate QWQ-llamacpp
</code></pre>
<p>【】</p>
<pre><code class="language-bash">conda install jupyterlab
conda install ipykernel
python -m ipykernel install --user --name QWQ --display-name &quot;Python QWQ&quot;
</code></pre>
<p><strong>llama.cpp</strong></p>
<p>【】</p>
<pre><code class="language-Bash">apt-get update
apt-get install build-essential cmake curl libcurl4-openssl-dev -y
</code></pre>
<p>【】</p>
<pre><code class="language-Bash">cd /root
git clone https://github.com/ggerganov/llama.cpp
</code></pre>
<p>【】</p>
<pre><code class="language-Bash">cmake /root/llama.cpp -B /root/llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON
</code></pre>
<ul>
<li><strong><code>cmake</code></strong>：运行 CMake 工具，用于配置和生成构建文件。</li>
<li><strong><code>/root/llama.cpp</code></strong>：指定项目的源代码所在的目录。在这个例子中，<code>llama.cpp</code> 是项目的根目录。</li>
<li><strong><code>-B /root/llama.cpp/build</code></strong>：指定生成构建文件的目录。<code>-B</code> 参数表示<strong>构建目录</strong>，<code>llama.cpp/build</code> 是生成的构建目录。这是 CMake 将生成的文件存放的地方（例如 Makefile 或 Ninja 构建文件）。</li>
<li>同时还指定了一些编译选项：</li>
<li><strong>禁用共享库</strong>（<code>-DBUILD_SHARED_LIBS=OFF</code>），生成 <strong>静态库</strong>。</li>
<li><strong>启用 CUDA 支持</strong>（<code>-DGGML_CUDA=ON</code>），以便在有 GPU 的情况下使用 GPU 加速。</li>
<li><strong>启用 CURL 库支持</strong>（<code>-DLLAMA_CURL=ON</code>），以便支持网络请求</li>
</ul>
<pre><code class="language-Bash">cmake --build /root/llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-server
</code></pre>
<ul>
<li><strong><code>--build /root/llama.cpp/build</code></strong>：告诉 CMake 使用 <code>llama.cpp/build</code> 目录中的构建文件来执行构建过程。这个目录是在之前运行 <code>cmake llama.cpp -B llama.cpp/build</code> 命令时生成的，包含了所有构建所需的文件（例如 Makefile 或 Ninja 构建文件）。</li>
<li><strong><code>--config Release</code></strong>：指定构建的配置为 <strong>Release</strong> 配置。</li>
<li><strong>Release</strong> 配置通常意味着启用更多的 <strong>优化</strong>，生成的程序运行速度较快，适合发布。</li>
<li>在 CMake 中，通常有两种常见的构建配置：<ul>
<li><strong>Debug</strong>：用于调试版本，包含调试信息且没有做过多优化。</li>
<li><strong>Release</strong>：优化后的发布版本，去除调试信息，运行时性能更高。</li>
</ul>
</li>
<li><strong><code>-j</code></strong>：表示并行构建，允许 CMake 使用多个 CPU 核心来加速构建过程。</li>
<li>如果没有指定数字，CMake 会使用默认的并行级别，通常是可用的所有 CPU 核心。你也可以指定并行的作业数，例如 <code>-j 8</code> 表示使用 8 个并行作业进行编译。</li>
<li><strong><code>--clean-first</code></strong>：表示在构建之前先清理掉之前的构建结果。这可以确保每次构建时都是从一个干净的状态开始，避免由于缓存或中间文件引起的编译错误。</li>
<li>如果你之前运行过构建并且有问题，或者希望重新构建而不使用任何缓存文件，这个选项非常有用。</li>
<li><strong><code>--target</code></strong>：指定构建的目标（target）。通常，一个项目会定义多个目标（比如库、可执行文件等），通过这个参数可以告诉 CMake 只编译特定的目标。</li>
<li><strong><code>llama-quantize</code></strong>：可能是与模型量化相关的目标。量化（quantization）是将模型的精度从浮点数降低到整数，从而减少内存占用和提高推理速度。</li>
<li><strong><code>llama-cli</code></strong>：可能是一个命令行工具，用于运行模型或与用户交互。</li>
<li><strong><code>llama-gguf-split</code></strong>：可能是一个用于拆分模型文件的目标，通常用于将一个大模型文件拆分成多个小文件，方便存储和加载。</li>
</ul>
<p>【】</p>
<pre><code class="language-bash"># 将工具目录加入PATH
echo 'export PATH=&quot;/root/llama.cpp/build/bin:$PATH&quot;' &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
<pre><code class="language-bash">./llama-cli \
  --model &lt;模型路径&gt; \         # 必需，指定模型文件（GGUF 格式）
  --threads &lt;数值&gt; \          # CPU 线程数（建议设为物理核心数）
  --n-gpu-layers &lt;数值&gt; \     # 启用 GPU 加速的层数（如 64）
  --ctx-size &lt;数值&gt; \         # 上下文窗口大小（如 4096）
  --temp &lt;数值&gt; \             # 温度参数（0.1~1.0，越高越随机）
  --seed &lt;数值&gt; \             # 随机种子（固定输出）
  --prompt &quot;输入内容&quot;         # 直接输入问题
</code></pre>
<pre><code class="language-bash">./llama-server \
  --model &lt;模型路径&gt; \         # 必需
  --host 0.0.0.0 \           # 允许外部访问
  --port 8000 \               # 端口号
  --n-gpu-layers 64 \         # GPU 加速层数
  --ctx-size 4096 \           # 上下文长度
  --parallel 4 \              # 并行请求数
  --verbose                   # 显示详细日志
</code></pre>
<p>【直接测试】</p>
<pre><code class="language-bash"> llama-cli -m QwQ-32B-Q4_K_M.gguf
</code></pre>
<p>【开启API服务】</p>
<pre><code class="language-bash">sudo lsof -i:8000
sudo lsof -i:8090
 ss -tule

 watch -n 1 &quot;nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv&quot;

 http://203.3.112.84:8000/
</code></pre>
<pre><code class="language-bash">llama-server \
    --model /root/QWQ-32B.gguf/QwQ-32B-Q4_K_M.gguf \
    --host 0.0.0.0 \
    --port 8000 \
    --cache-type-k q4_0 \
    --threads 64 \
    --prio 2 \
    --temp 0.6 \
    --ctx-size 2048 \
    --seed 3407 \
    --n-gpu-layers 64 \
    --repeat-penalty 1.1 \
</code></pre>
<pre><code class="language-bash">nohup llama-server \
    --model /root/QWQ-32B.gguf/QwQ-32B-Q4_K_M.gguf \
    --host 0.0.0.0 \
    --port 8000 \
    --threads 16 \               # 调整为物理核心数（而非逻辑线程数）
    --ctx-size 2048 \            # 扩大上下文窗口避免频繁shift
    --n-gpu-layers 40 \          # 根据实际显存调整（建议从40开始测试）
    --temp 0.6 \
    --repeat-penalty 1.1 \       # 新增防止重复生成
    --n-predict 512 \            # 新增限制最大生成长度
    --cache-type-k f16 \         # 更匹配Q4_K_M模型的缓存类型
    --mlock \                    # 锁定模型在内存中
    --keepalive 300 \            # 保持连接防止超时
    --log-format json \          # 结构化日志
    --verbose 2&gt;&amp;1 | tee server.log &amp;


nohup llama-server \
    --model /root/QWQ-32B.gguf/QwQ-32B-Q4_K_M.gguf \
    --host 0.0.0.0 \
    --port 8000 \
    --threads 16 \
    --ctx-size 2048 \
    --n-gpu-layers 40 \
    --temp 0.6 \
    --repeat-penalty 1.1 \
    --n-predict 512 \
    --cache-type-k f16 \
    --mlock \
    --verbose 2&gt;&amp;1 | tee server.log &amp;



    nohup llama-server \
    --model /root/QWQ-32B.gguf/QwQ-32B-Q4_K_M.gguf \
    --host 0.0.0.0 \
    --port 8000 \
    --threads 16 \
    --ctx-size 2048 \
    --n-gpu-layers 40 \
    --temp 0.6 \
    --repeat-penalty 1.1 \
    --n-predict 512 \
    --cache-type-k f16 \
    --mlock \
    --verbose \
    --cont-batching \            # 启用连续批处理提升吞吐
    --slot-prompt-similarity 0.8 \  # 提高slot匹配阈值防止误触发
    --timeout 300 \              # 延长超时时间到5分钟
    --no-context-shift \         # 禁用自动上下文切换
    --chat-template deepseek3 \  # 指定与模型匹配的对话模板
    --sampler-seq kpmdxt \       # 优化采样器顺序
    --metrics \                  # 启用监控端点
    --tensor-split 24,24 \       # 双4090显存分配
    --main-gpu 0 2&gt;&amp;1 | tee server.log &amp;



nohup llama-server \
    --model /root/QWQ-32B.gguf/QwQ-32B-Q4_K_M.gguf \
    --host 0.0.0.0 \
    --port 8000 \
    --threads 16 \
    --ctx-size 4096 \
    --n-gpu-layers 40 \
    --temp 0.6 \
    --repeat-penalty 1.1 \
    --n-predict 512 \
    --cache-type-k f16 \
    --mlock \
    --verbose \
    --cont-batching \
    --slot-prompt-similarity 0.8 \
    --timeout 300 \
    --no-context-shift \
    --chat-template deepseek3 \
    --sampler-seq kpmdxt \
    --metrics \
    --tensor-split 24,24 \
    --main-gpu 0 2&gt;&amp;1 | tee server.log &amp;
</code></pre>
<p>根据您提供的 <code>llama-server --help</code> 输出，以下是核心参数的分类解析和实用场景说明（针对您的双 RTX 4090 配置优化）：</p>
<hr />
<h3 id="_1"><strong>一、模型加载相关</strong></h3>
<table>
<thead>
<tr>
<th>参数</th>
<th>示例值</th>
<th>说明</th>
<th>使用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--model</code></td>
<td><code>/path/to/model.gguf</code></td>
<td><strong>必填</strong> 模型文件路径</td>
<td>加载指定模型</td>
</tr>
<tr>
<td><code>--n-gpu-layers</code></td>
<td>40</td>
<td>GPU 加速层数</td>
<td>平衡 GPU/CPU 负载（RTX4090 建议 40-64）</td>
</tr>
<tr>
<td><code>--tensor-split</code></td>
<td>24,24</td>
<td>GPU 显存分配</td>
<td>双显卡时指定每卡显存（单位 GB）</td>
</tr>
<tr>
<td><code>--main-gpu</code></td>
<td>0</td>
<td>主 GPU 编号</td>
<td>多 GPU 时指定主计算卡</td>
</tr>
<tr>
<td><code>--mlock</code></td>
<td>-</td>
<td>锁定模型到内存</td>
<td>避免交换，需足够 RAM</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="_2"><strong>二、性能优化</strong></h3>
<table>
<thead>
<tr>
<th>参数</th>
<th>示例值</th>
<th>说明</th>
<th>使用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--ctx-size</code></td>
<td>2048</td>
<td>上下文窗口大小</td>
<td>对话历史长度（建议 2048-4096）</td>
</tr>
<tr>
<td><code>--threads</code></td>
<td>16</td>
<td>CPU 线程数</td>
<td>建议设为物理核心数（非超线程数）</td>
</tr>
<tr>
<td><code>--cont-batching</code></td>
<td>-</td>
<td>连续批处理</td>
<td><strong>高并发必开</strong>，提升吞吐量</td>
</tr>
<tr>
<td><code>--ubatch-size</code></td>
<td>512</td>
<td>物理批处理大小</td>
<td>根据显存调整（4090 可设 512-1024）</td>
</tr>
<tr>
<td><code>--cache-type-k</code></td>
<td>f16</td>
<td>K 缓存数据类型</td>
<td>量化模型建议 f16/q8_0</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="_3"><strong>三、采样控制</strong></h3>
<table>
<thead>
<tr>
<th>参数</th>
<th>示例值</th>
<th>说明</th>
<th>优化技巧</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--temp</code></td>
<td>0.6</td>
<td>温度系数</td>
<td>越高越随机（0.2-0.8）</td>
</tr>
<tr>
<td><code>--repeat-penalty</code></td>
<td>1.1</td>
<td>重复惩罚</td>
<td>抑制重复内容（1.0-1.2）</td>
</tr>
<tr>
<td><code>--n-predict</code></td>
<td>512</td>
<td>最大生成 token 数</td>
<td>防止无限生成</td>
</tr>
<tr>
<td><code>--top-k</code></td>
<td>40</td>
<td>候选 token 数</td>
<td>越高多样性越强</td>
</tr>
<tr>
<td><code>--mirostat</code></td>
<td>2</td>
<td>智能采样模式</td>
<td>推荐 Mirostat 2.0</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="api"><strong>四、API 服务配置</strong></h3>
<table>
<thead>
<tr>
<th>参数</th>
<th>示例值</th>
<th>说明</th>
<th>安全建议</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--host</code></td>
<td>0.0.0.0</td>
<td>监听地址</td>
<td>外网访问需设置</td>
</tr>
<tr>
<td><code>--port</code></td>
<td>8000</td>
<td>服务端口</td>
<td>避免使用 80/443</td>
</tr>
<tr>
<td><code>--api-key</code></td>
<td>your_key</td>
<td>API 密钥</td>
<td>生产环境必设</td>
</tr>
<tr>
<td><code>--timeout</code></td>
<td>300</td>
<td>请求超时（秒）</td>
<td>根据模型速度调整</td>
</tr>
<tr>
<td><code>--slot-prompt-similarity</code></td>
<td>0.8</td>
<td>请求相似度阈值</td>
<td><strong>防误触发关键参数</strong></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="_4"><strong>五、高级调试</strong></h3>
<table>
<thead>
<tr>
<th>参数</th>
<th>示例值</th>
<th>说明</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--verbose</code></td>
<td>-</td>
<td>详细日志</td>
<td>调试时启用</td>
</tr>
<tr>
<td><code>--metrics</code></td>
<td>-</td>
<td>启用监控端点</td>
<td>Prometheus 指标采集</td>
</tr>
<tr>
<td><code>--log-file</code></td>
<td>server.log</td>
<td>日志文件</td>
<td>持久化日志</td>
</tr>
<tr>
<td><code>--override-kv</code></td>
<td><code>tokenizer.ggml.add_bos_token=bool:false</code></td>
<td>覆盖模型元数据</td>
<td>修复特殊模型兼容性</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="gpu"><strong>六、多 GPU 优化示例</strong></h3>
<pre><code class="language-bash">llama-server \
  --model /root/QWQ-32B.gguf/QwQ-32B-Q4_K_M.gguf \
  --n-gpu-layers 48 \
  --tensor-split 24,24 \    # 双卡各分配24G显存
  --main-gpu 0 \            # 主卡负责调度
  --cont-batching \         # 启用批处理
  --ctx-size 4096 \         # 长上下文支持
  --ubatch-size 1024 \      # 大批次提升吞吐
  --slot-prompt-similarity 0.85  # 严格匹配请求
</code></pre>
<hr />
<h3 id="_5"><strong>七、监控建议</strong></h3>
<ol>
<li>
<p><strong>显存监控</strong>：
   <code>bash
   watch -n 1 "nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv"</code></p>
</li>
<li>
<p><strong>API 健康检查</strong>：
   <code>bash
   curl -s http://localhost:8000/metrics | grep "llama_requests_total"</code></p>
</li>
<li>
<p><strong>性能分析</strong>：
   <code>bash
   perf stat -d -p $(pgrep llama-server)</code></p>
</li>
</ol>
<p>根据实际负载动态调整 <code>--ubatch-size</code> 和 <code>--threads</code> 参数可获得最佳性价比。如果遇到中文输出异常，可尝试添加 <code>--override-kv tokenizer.ggml.add_bos_token=bool:false</code> 参数。 </p>
<p>【纯GPU推理】</p>
<pre><code class="language-bash">llama-cli \
    --model /root/QWQ-32B.gguf/QwQ-32B-Q4_K_M.gguf \
    --cache-type-k q4_0 \
    --threads 64 \
    --prio 2 \
    --temp 0.6 \
    --ctx-size 512 \
    --seed 3407 \
    --n-gpu-layers 64 \
    -no-cnv \
    --prompt &quot;&lt;｜User｜&gt;你好，好久不见，请介绍下你自己。&lt;｜Assistant｜&gt;&quot; 
</code></pre>
<p>其中命令行核心参数说明：</p>
<ul>
<li><code>--threads</code>：CPU 核心数;</li>
<li><code>--ctx-size</code>：输出的上下文长度；</li>
<li><code>--n-gpu-layers</code> ：需要卸载到 GPU 的层数，设置为0时代表完全使用CPU进行推理；</li>
<li><code>--temp</code>：模型温度参数；</li>
<li><code>-no-cnv</code>：不进行多轮对话；</li>
<li><code>--cache-type-k</code>：K 缓存量化为 4bit；</li>
<li><code>--seed</code>：随机数种子；</li>
</ul>
<p>【设置OpenWebUI】</p>
<pre><code class="language-Bash">pip install open-webui
export HF_HUB_OFFLINE=1
open-webui serve
</code></pre>
<pre><code class="language-bash">ssh -CNgv -L 8090:127.0.0.1:8090 root@203.3.112.84 -p 22
</code></pre>
<pre><code class="language-bash">docker run -d \
  --name openwebui \
  --network host \
  -p 8090:8080 \
  -e OLLAMA_API_BASE_URL=http://host.docker.internal:8000 \
  -v ~/openwebui_data:/app/backend/data \
  --add-host=host.docker.internal:host-gateway \
  ghcr.io/open-webui/open-webui:main



  docker run -d \
  --name openwebui \
  --network host \
  -p 8090:8080 \
  -e OLLAMA_API_BASE_URL=http://127.0.0.1:8000 \
  -v ~/openwebui_data:/app/backend/data \
  ghcr.io/open-webui/open-webui:main

  ss -tlnp | grep 8000
  netstat -tulnp | grep 8090

  # 1. 进入容器测试
docker exec -it openwebui /bin/bash
exit
curl -v http://127.0.0.1:8000/v1/models
curl -v http://203.3.112.84:8000/v1/models
http://203.3.112.84:8000/v1

# 2. 检查端口映射
docker port openwebui  # 应无输出（host 模式直接使用宿主机端口）

# 3. 浏览器访问
http://服务器IP:8080  # host 模式直接使用 8080 端口


docker run -d \
  --name openwebui \
  -p 8090:8080 \
  -e OLLAMA_API_BASE_URL=http://host.docker.internal:8000 \
  -v ~/openwebui_data:/app/backend/data \
  --add-host=host.docker.internal:host-gateway \
  ghcr.io/open-webui/open-webui:main


</code></pre>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>